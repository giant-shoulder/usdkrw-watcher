from curl_cffi import requests # ë³€ê²½
from bs4 import BeautifulSoup
import re
from datetime import datetime
import pytz
import time as pytime
from typing import Optional

def fetch_expected_range():
    # í—¤ë”ëŠ” ê·¸ëŒ€ë¡œ ë‘ê±°ë‚˜ ìµœì†Œí™”í•´ë„ ë¨ (impersonateê°€ ì•Œì•„ì„œ ì²˜ë¦¬í•¨)
    headers = {
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Referer": "https://news.einfomax.co.kr/",
    }
    
    search_url = (
        "https://news.einfomax.co.kr/news/articleList.html?sc_area=A&view_type=sm&sc_word=%ED%99%98%EC%9C%A8+%EC%98%88%EC%83%81+%EB%A0%88%EC%9D%B8%EC%A7%80"
    )

    # requests.Session() ëŒ€ì‹  curl_cffi ì‚¬ìš©
    session = requests.Session()

    def _get(url: str, *, timeout: int = 15, retries: int = 3):
        last_err: Optional[Exception] = None
        for i in range(retries):
            try:
                # impersonate="chrome" ì˜µì…˜ì´ í•µì‹¬ìž…ë‹ˆë‹¤.
                r = session.get(
                    url, 
                    headers=headers, 
                    impersonate="chrome", 
                    timeout=timeout, 
                    allow_redirects=True
                )
                if r.status_code >= 400:
                    r.raise_for_status()
                return r
            except Exception as e:
                last_err = e
                print(f"Retry {i+1} failed: {e}")
                pytime.sleep(1)
        raise last_err

    # ... (ë‚˜ë¨¸ì§€ ë¡œì§ì€ ë™ì¼) ...
    res = _get(search_url)
    
    # ë””ë²„ê¹…ìš© ë¡œê·¸ (ë°°í¬ í™˜ê²½ì—ì„œ í™•ì¸ìš©)
    if "ì˜ˆìƒ" not in res.text and "ë ˆì¸ì§€" not in res.text:
        print(f"âš ï¸ ê²½ê³ : ê²€ìƒ‰ ê²°ê³¼ íŽ˜ì´ì§€ê°€ ì˜ì‹¬ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. Status: {res.status_code}")
        # print(res.text[:500]) # HTML ì•žë¶€ë¶„ í™•ì¸
    res.raise_for_status()
    soup = BeautifulSoup(res.text, "html.parser")

    # 2. ìµœì‹  ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ (ë°°í¬ í™˜ê²½ì—ì„œ HTML êµ¬ì¡°/ì°¨ë‹¨ íŽ˜ì´ì§€ ëŒ€ì‘)
    article_tag = (
        soup.select_one("ul.type2 li a")
        or soup.select_one("ul.type1 li a")
        or soup.select_one("div#section-list li a")
        or soup.select_one("div.list li a")
        or soup.select_one("div.listing li a")
    )

    href = article_tag.get("href") if article_tag else None
    if not href:
        # Railway/í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ ì°¨ë‹¨/ë¦¬ë‹¤ì´ë ‰íŠ¸/ë¹„ì •ìƒ HTMLì¸ì§€ ë¹ ë¥´ê²Œ í™•ì¸í•  ìˆ˜ ìžˆê²Œ ì¼ë¶€ ì¶œë ¥
        snippet = soup.get_text("\n", strip=True)[:400]
        print("[EXPECTED_RANGE] search page status=", res.status_code)
        print("[EXPECTED_RANGE] search page snippet=", snippet)
        raise ValueError("âŒ ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ì ˆëŒ€/ìƒëŒ€ URL ëª¨ë‘ ì²˜ë¦¬
    if href.startswith("http"):
        article_url = href
    else:
        article_url = "https://news.einfomax.co.kr" + href

    article_res = _get(article_url)
    article_res.raise_for_status()
    article_soup = BeautifulSoup(article_res.text, "html.parser")

    def _extract_article_text(soup: BeautifulSoup) -> str:
        """Extract main article text as reliably as possible.

        Einfomax pages sometimes include a lot of navigation/boilerplate; also some environments
        may receive a 'block/interstitial' HTML with 200. We try common article containers first.
        """
        candidates = [
            "div#article-view-content-div",          # common on many Korean news CMS
            "div#articleBody",                      # fallback
            "section#article-view-content-div",     # variant
            "div.article-body",                     # generic
            "div.view_cont",                        # generic
            "article",                              # last resort
        ]
        for sel in candidates:
            el = soup.select_one(sel)
            if el:
                txt = el.get_text("\n", strip=True)
                if txt and len(txt) > 200:
                    return txt
        return soup.get_text("\n", strip=True)

    def _debug_context(text: str, keyword: str, width: int = 200) -> str:
        i = text.find(keyword)
        if i < 0:
            return ""
        start = max(0, i - width)
        end = min(len(text), i + len(keyword) + width)
        return text[start:end]

    body_text = _extract_article_text(article_soup)

    # ë°°í¬ í™˜ê²½ì—ì„œ ì¢…ì¢… 200ìœ¼ë¡œ ì°¨ë‹¨/ì•ˆë‚´ íŽ˜ì´ì§€ê°€ ë‚´ë ¤ì˜¤ëŠ” ê²½ìš°ê°€ ìžˆì–´, ë³¸ë¬¸ì´ ë¹„ì •ìƒì ìœ¼ë¡œ ì§§ìœ¼ë©´ ì°¨ë‹¨ ì˜ì‹¬
    if len(body_text) < 300:
        print("[EXPECTED_RANGE] article page status=", article_res.status_code)
        print("[EXPECTED_RANGE] article page snippet=", body_text[:400])

    # Railwayì—ì„œë§Œ ìž¬í˜„ë˜ëŠ” 'ì •ìƒ 200ì¸ë° ë‚´ìš©ì´ ë‹¤ë¥¸' ì¼€ì´ìŠ¤ë¥¼ ë¹ ë¥´ê²Œ íŒë³„
    if ("ì ‘ê·¼" in body_text and "ì°¨ë‹¨" in body_text) or ("Forbidden" in body_text) or ("Cloudflare" in body_text):
        print("[EXPECTED_RANGE] âš ï¸ possible block/interstitial page detected")

    # 4. ê¸°ì‚¬ ë‚ ì§œ í™•ì¸
    meta_time = article_soup.find("meta", {"property": "article:published_time"})
    if not meta_time or not meta_time.get("content"):
        raise ValueError("âŒ ê¸°ì‚¬ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    article_date = datetime.strptime(meta_time["content"].split("T")[0], "%Y-%m-%d").date()

    today = datetime.now(pytz.timezone("Asia/Seoul")).date()
    if article_date != today:
        raise ValueError(f"ðŸ“… ì˜¤ëŠ˜ ê¸°ì‚¬ ì•„ë‹˜: {article_date}")

    # 5. ì „ì²´ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    full_text = body_text

    # 6. ì •ê·œì‹ìœ¼ë¡œ ì˜ˆìƒ ë ˆì¸ì§€ ì¶”ì¶œ (í‘œê¸° ë³€í˜• ëŒ€ì‘)
    # - 'ì˜ˆìƒ ë ˆì¸ì§€', 'ì˜ˆìƒë ˆì¸ì§€', 'ì˜ˆìƒ ë²”ìœ„', 'ì˜ˆìƒí™˜ìœ¨ ë ˆì¸ì§€' ë“±
    # - êµ¬ë¶„ìž: ~, -, â€“
    # - ë‹¨ìœ„: 'ì›' ìœ ë¬´
    patterns = [
        r"ì˜ˆìƒ\s*(?:í™˜ìœ¨\s*)?(?:ë ˆì¸ì§€|ë²”ìœ„)\s*[:ï¼š]?\s*([\d,\.]+)\s*[~\-â€“]\s*([\d,\.]+)\s*ì›?",
        r"(?:ë ˆì¸ì§€|ë²”ìœ„)\s*[:ï¼š]?\s*([\d,\.]+)\s*[~\-â€“]\s*([\d,\.]+)\s*ì›?\s*(?:ë¡œ|ìœ¼ë¡œ)?\s*ì˜ˆìƒ",
    ]

    range_matches = []
    for pat in patterns:
        found = re.findall(pat, full_text)
        if found:
            range_matches.extend(found)

    if not range_matches:
        # Debug: Railwayì—ì„œ ë³¸ë¬¸ì€ ë°›ì•„ì™”ëŠ”ë° í‚¤ì›Œë“œ/í˜•ì‹ì´ ë‹¬ë¼ ì‹¤íŒ¨í•˜ëŠ” ì¼€ì´ìŠ¤
        ctx1 = _debug_context(full_text, "ì˜ˆìƒ", 250)
        ctx2 = _debug_context(full_text, "ë ˆì¸ì§€", 250)
        ctx3 = _debug_context(full_text, "ë²”ìœ„", 250)
        print("[EXPECTED_RANGE] âŒ regex miss - contexts:")
        if ctx1:
            print("[EXPECTED_RANGE] ...around 'ì˜ˆìƒ'...\n", ctx1)
        if ctx2:
            print("[EXPECTED_RANGE] ...around 'ë ˆì¸ì§€'...\n", ctx2)
        if ctx3:
            print("[EXPECTED_RANGE] ...around 'ë²”ìœ„'...\n", ctx3)

        # Fallback: í‚¤ì›Œë“œê°€ ìžˆëŠ” ê²½ìš°, ì£¼ë³€ì—ì„œ 'ìˆ«ìž~ìˆ«ìž' í˜•íƒœë¥¼ í•œ ë²ˆ ë” íƒìƒ‰
        window_text = "\n".join([t for t in [ctx1, ctx2, ctx3] if t]) or full_text
        fallback = re.findall(r"([\d,\.]{3,})\s*[~\-â€“]\s*([\d,\.]{3,})", window_text)
        if fallback:
            range_matches = fallback
        else:
            raise ValueError("âŒ ì˜ˆìƒ í™˜ìœ¨ ë²”ìœ„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 7. ì‰¼í‘œ ì œê±° ë° float ë³€í™˜
    ranges = []
    for low, high in range_matches:
        try:
            low_clean = float(low.replace(",", ""))
            high_clean = float(high.replace(",", ""))
            ranges.append((low_clean, high_clean))
        except ValueError:
            continue

    if not ranges:
        raise ValueError("âŒ ìœ íš¨í•œ ìˆ«ìž í˜•ì‹ì˜ ë²”ìœ„ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # 8. ê°€ìž¥ ë„“ì€ ë²”ìœ„ ê³„ì‚°
    low = min(l for l, _ in ranges)
    high = max(h for _, h in ranges)

    print("âœ… ìŠ¤í¬ëž˜í•‘ëœ ì˜ˆìƒ í™˜ìœ¨ ë ˆì¸ì§€:", ranges)

    return {
        "date": today,
        "low": low,
        "high": high,
        "source": article_url,
    }