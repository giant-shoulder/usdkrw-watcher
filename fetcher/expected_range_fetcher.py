from curl_cffi import requests # ë³€ê²½
from bs4 import BeautifulSoup
import re
from datetime import datetime
import pytz
import time as pytime
from typing import Optional

def fetch_expected_range():
    # í—¤ë”ëŠ” ê·¸ëŒ€ë¡œ ë‘ê±°ë‚˜ ìµœì†Œí™”í•´ë„ ë¨ (impersonateê°€ ì•Œì•„ì„œ ì²˜ë¦¬í•¨)
    headers = {
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Referer": "https://news.einfomax.co.kr/",
    }
    
    search_url = (
        "https://news.einfomax.co.kr/news/articleList.html?sc_area=A&view_type=sm&sc_word=%ED%99%98%EC%9C%A8+%EC%98%88%EC%83%81+%EB%A0%88%EC%9D%B8%EC%A7%80"
    )

    # requests.Session() ëŒ€ì‹  curl_cffi ì‚¬ìš©
    session = requests.Session()

    def _get(url: str, *, timeout: int = 15, retries: int = 3):
        last_err: Optional[Exception] = None
        for i in range(retries):
            try:
                # impersonate="chrome" ì˜µì…˜ì´ í•µì‹¬ì…ë‹ˆë‹¤.
                r = session.get(
                    url, 
                    headers=headers, 
                    impersonate="chrome", 
                    timeout=timeout, 
                    allow_redirects=True
                )
                if r.status_code >= 400:
                    r.raise_for_status()
                return r
            except Exception as e:
                last_err = e
                print(f"Retry {i+1} failed: {e}")
                pytime.sleep(1)
        raise last_err

    # ... (ë‚˜ë¨¸ì§€ ë¡œì§ì€ ë™ì¼) ...
    res = _get(search_url)
    
    # ë””ë²„ê¹…ìš© ë¡œê·¸ (ë°°í¬ í™˜ê²½ì—ì„œ í™•ì¸ìš©)
    if "ì˜ˆìƒ" not in res.text and "ë ˆì¸ì§€" not in res.text:
        print(f"âš ï¸ ê²½ê³ : ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ê°€ ì˜ì‹¬ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. Status: {res.status_code}")
        # print(res.text[:500]) # HTML ì•ë¶€ë¶„ í™•ì¸
    res.raise_for_status()
    soup = BeautifulSoup(res.text, "html.parser")

    # 2. ìµœì‹  ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ (ë°°í¬ í™˜ê²½ì—ì„œ HTML êµ¬ì¡°/ì°¨ë‹¨ í˜ì´ì§€ ëŒ€ì‘)
    article_tag = (
        soup.select_one("ul.type2 li a")
        or soup.select_one("ul.type1 li a")
        or soup.select_one("div#section-list li a")
        or soup.select_one("div.list li a")
        or soup.select_one("div.listing li a")
    )

    href = article_tag.get("href") if article_tag else None
    if not href:
        # Railway/í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ ì°¨ë‹¨/ë¦¬ë‹¤ì´ë ‰íŠ¸/ë¹„ì •ìƒ HTMLì¸ì§€ ë¹ ë¥´ê²Œ í™•ì¸í•  ìˆ˜ ìˆê²Œ ì¼ë¶€ ì¶œë ¥
        snippet = soup.get_text("\n", strip=True)[:400]
        print("[EXPECTED_RANGE] search page status=", res.status_code)
        print("[EXPECTED_RANGE] search page snippet=", snippet)
        raise ValueError("âŒ ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ì ˆëŒ€/ìƒëŒ€ URL ëª¨ë‘ ì²˜ë¦¬
    if href.startswith("http"):
        article_url = href
    else:
        article_url = "https://news.einfomax.co.kr" + href

    article_res = _get(article_url)
    article_res.raise_for_status()
    article_soup = BeautifulSoup(article_res.text, "html.parser")

    # ë°°í¬ í™˜ê²½ì—ì„œ ì¢…ì¢… 200ìœ¼ë¡œ ì°¨ë‹¨ í˜ì´ì§€ê°€ ë‚´ë ¤ì˜¤ëŠ” ê²½ìš°ê°€ ìˆì–´, ë³¸ë¬¸ì´ ë¹„ì •ìƒì ìœ¼ë¡œ ì§§ìœ¼ë©´ ì°¨ë‹¨ ì˜ì‹¬
    body_text = article_soup.get_text("\n", strip=True)
    if len(body_text) < 300:
        print("[EXPECTED_RANGE] article page status=", article_res.status_code)
        print("[EXPECTED_RANGE] article page snippet=", body_text[:400])

    # 4. ê¸°ì‚¬ ë‚ ì§œ í™•ì¸
    meta_time = article_soup.find("meta", {"property": "article:published_time"})
    if not meta_time or not meta_time.get("content"):
        raise ValueError("âŒ ê¸°ì‚¬ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    article_date = datetime.strptime(meta_time["content"].split("T")[0], "%Y-%m-%d").date()

    today = datetime.now(pytz.timezone("Asia/Seoul")).date()
    if article_date != today:
        raise ValueError(f"ğŸ“… ì˜¤ëŠ˜ ê¸°ì‚¬ ì•„ë‹˜: {article_date}")

    # 5. ì „ì²´ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    full_text = body_text

    # 6. ì •ê·œì‹ìœ¼ë¡œ ì˜ˆìƒ ë ˆì¸ì§€ ì¶”ì¶œ (ì‰¼í‘œ í¬í•¨ ìˆ«ì ëŒ€ì‘)
    range_matches = re.findall(
        r"ì˜ˆìƒ\s*ë ˆì¸ì§€\s*[:ï¼š]?\s*([\d,\.]+)\s*[~\-]\s*([\d,\.]+)",
        full_text
    )
    if not range_matches:
        raise ValueError("âŒ ì˜ˆìƒ í™˜ìœ¨ ë²”ìœ„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 7. ì‰¼í‘œ ì œê±° ë° float ë³€í™˜
    ranges = []
    for low, high in range_matches:
        try:
            low_clean = float(low.replace(",", ""))
            high_clean = float(high.replace(",", ""))
            ranges.append((low_clean, high_clean))
        except ValueError:
            continue

    if not ranges:
        raise ValueError("âŒ ìœ íš¨í•œ ìˆ«ì í˜•ì‹ì˜ ë²”ìœ„ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # 8. ê°€ì¥ ë„“ì€ ë²”ìœ„ ê³„ì‚°
    low = min(l for l, _ in ranges)
    high = max(h for _, h in ranges)

    print("âœ… ìŠ¤í¬ë˜í•‘ëœ ì˜ˆìƒ í™˜ìœ¨ ë ˆì¸ì§€:", ranges)

    return {
        "date": today,
        "low": low,
        "high": high,
        "source": article_url,
    }