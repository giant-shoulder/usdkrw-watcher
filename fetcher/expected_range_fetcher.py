from curl_cffi import requests # ë³€ê²½
from bs4 import BeautifulSoup
import re
from datetime import datetime
import pytz
import time as pytime
from typing import Optional

def fetch_expected_range():
    # í—¤ë”ëŠ” ê·¸ëŒ€ë¡œ ë‘ê±°ë‚˜ ìµœì†Œí™”í•´ë„ ë¨ (impersonateê°€ ì•Œì•„ì„œ ì²˜ë¦¬í•¨)
    headers = {
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Referer": "https://news.einfomax.co.kr/",
    }
    
    search_url = (
        "https://news.einfomax.co.kr/news/articleList.html?sc_area=A&view_type=sm&sc_word=%ED%99%98%EC%9C%A8+%EC%98%88%EC%83%81"
    )

    # requests.Session() ëŒ€ì‹  curl_cffi ì‚¬ìš©
    session = requests.Session()

    def _get(url: str, *, timeout: int = 15, retries: int = 3):
        last_err: Optional[Exception] = None
        for i in range(retries):
            try:
                # impersonate="chrome" ì˜µì…˜ì´ í•µì‹¬ìž…ë‹ˆë‹¤.
                r = session.get(
                    url, 
                    headers=headers, 
                    impersonate="chrome", 
                    timeout=timeout, 
                    allow_redirects=True
                )
                if r.status_code >= 400:
                    r.raise_for_status()
                return r
            except Exception as e:
                last_err = e
                print(f"Retry {i+1} failed: {e}")
                pytime.sleep(1)
        raise last_err

    # ... (ë‚˜ë¨¸ì§€ ë¡œì§ì€ ë™ì¼) ...
    res = _get(search_url)
    
    # ë””ë²„ê¹…ìš© ë¡œê·¸ (ë°°í¬ í™˜ê²½ì—ì„œ í™•ì¸ìš©)
    if "ì˜ˆìƒ" not in res.text and "ë ˆì¸ì§€" not in res.text:
        print(f"âš ï¸ ê²½ê³ : ê²€ìƒ‰ ê²°ê³¼ íŽ˜ì´ì§€ê°€ ì˜ì‹¬ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. Status: {res.status_code}")
        # print(res.text[:500]) # HTML ì•žë¶€ë¶„ í™•ì¸
    res.raise_for_status()
    soup = BeautifulSoup(res.text, "html.parser")

    # 2. ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê¸°ì‚¬ í›„ë³´ ë§í¬ ì—¬ëŸ¬ ê°œ ìˆ˜ì§‘ (ë°°í¬ í™˜ê²½ì—ì„œ ì²« ë²ˆì§¸ê°€ ë¬´ê´€/ìœ ë£Œ(ë‹¨ë§ê¸°) ê¸°ì‚¬ì¼ ìˆ˜ ìžˆìŒ)
    def _normalize_article_url(href: str) -> str:
        if not href:
            return ""
        href = href.strip()
        if href.startswith("http"):
            return href
        return "https://news.einfomax.co.kr" + href

    # ê°€ëŠ¥í•œ ëª©ë¡ ì…€ë ‰í„°ë“¤ì—ì„œ a[href]ë¥¼ ìµœëŒ€í•œ ë§Žì´ ëª¨ì€ë‹¤.
    link_selectors = [
        "ul.type2 li a[href]",
        "ul.type1 li a[href]",
        "div#section-list li a[href]",
        "div.list li a[href]",
        "div.listing li a[href]",
        "div.article-list a[href]",
        "a[href*='/news/articleView.html']",
    ]

    candidates: list[str] = []
    for sel in link_selectors:
        for a in soup.select(sel):
            href = a.get("href")
            url = _normalize_article_url(href)
            if not url:
                continue
            # ì¤‘ë³µ ì œê±°
            if url not in candidates:
                candidates.append(url)

    if not candidates:
        snippet = soup.get_text("\n", strip=True)[:400]
        print("[EXPECTED_RANGE] search page status=", res.status_code)
        print("[EXPECTED_RANGE] search page snippet=", snippet)
        raise ValueError("âŒ ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ê²€ìƒ‰ ê²°ê³¼ íŽ˜ì´ì§€ê°€ ì˜ì‹¬ìŠ¤ëŸ¬ìš¸ ë•Œ(í‚¤ì›Œë“œ ë¯¸í¬í•¨/ì°¨ë‹¨ HTML) í›„ë³´ë¥¼ ë” ë„“ê²Œ ìž¡ë˜ ë¡œê·¸ ë‚¨ê¹€
    if "ì˜ˆìƒ" not in res.text and "ë ˆì¸ì§€" not in res.text and "ë²”ìœ„" not in res.text:
        print(f"âš ï¸ ê²½ê³ : ê²€ìƒ‰ ê²°ê³¼ íŽ˜ì´ì§€ê°€ ì˜ì‹¬ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. Status: {res.status_code}")

    # 3. í›„ë³´ ê¸°ì‚¬ë“¤ì„ ìˆœíšŒí•˜ë©° 'ì˜ˆìƒ ë ˆì¸ì§€/ë²”ìœ„' íŒ¨í„´ì´ ì‹¤ì œë¡œ ì¡´ìž¬í•˜ëŠ” ê¸°ì‚¬ë§Œ ì±„íƒ
    #    (ìœ ë£Œ ë‹¨ë§ê¸° ì•ˆë‚´ ë¬¸êµ¬/ë¬´ê´€ ê¸°ì‚¬/ì°¨ë‹¨ íŽ˜ì´ì§€ëŠ” ìŠ¤í‚µ)
    PAYWALL_HINTS = [
        "ì¸í¬ë§¥ìŠ¤ ê¸ˆìœµì •ë³´ ë‹¨ë§ê¸°",
        "ë¬´ë‹¨ì „ìž¬",
        "AI í•™ìŠµ ë° í™œìš© ê¸ˆì§€",
    ]

    article_url = None
    article_soup = None
    body_text = None

    max_probe = min(12, len(candidates))
    for idx, url in enumerate(candidates[:max_probe], start=1):
        try:
            print(f"[EXPECTED_RANGE] probe {idx}/{max_probe}: {url}")
            r = _get(url)
            r.raise_for_status()
            s = BeautifulSoup(r.text, "html.parser")

            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            tmp_body = None
            try:
                tmp_body = _extract_article_text(s)
            except Exception:
                tmp_body = s.get_text("\n", strip=True)

            # ìœ ë£Œ/ë‹¨ë§ê¸° ì•ˆë‚´ íŽ˜ì´ì§€ëŠ” ìŠ¤í‚µ
            if any(hint in (tmp_body or "") for hint in PAYWALL_HINTS) and "ì˜ˆìƒ" not in (tmp_body or ""):
                print("[EXPECTED_RANGE] skip: paywall/terminal-only or irrelevant")
                continue

            # ë²”ìœ„ íŒ¨í„´ì„ ê¸°ì‚¬ë³„ë¡œ ì„ ê²€ì¦ (regexëŠ” ì•„ëž˜ì—ì„œ ë™ì¼ patternsë¡œ ìž¬ì‚¬ìš©)
            probe_text = tmp_body or ""
            probe_patterns = [
                r"ì˜ˆìƒ\s*(?:í™˜ìœ¨\s*)?(?:ë ˆì¸ì§€|ë²”ìœ„)",
                r"í™˜ìœ¨\s*ì˜ˆìƒ\s*(?:ë ˆì¸ì§€|ë²”ìœ„)",
            ]
            if not any(re.search(p, probe_text) for p in probe_patterns):
                print("[EXPECTED_RANGE] skip: keyword pattern not found")
                continue

            # í›„ë³´ ì±„íƒ
            article_url = url
            article_soup = s
            body_text = tmp_body
            break
        except Exception as e:
            print(f"[EXPECTED_RANGE] probe error: {type(e).__name__} - {e}")
            continue

    if not article_url or not article_soup:
        raise ValueError("âŒ ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì•˜ì§€ë§Œ, ì˜ˆìƒ ë ˆì¸ì§€/ë²”ìœ„ íŒ¨í„´ì´ ìžˆëŠ” ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    article_res = None  # ì•„ëž˜ ì½”ë“œì—ì„œ status/snippet ë¡œê¹…ìš© ë³€ìˆ˜ë¥¼ ìœ ì§€í•˜ë ¤ë©´ None ì²˜ë¦¬

    def _extract_article_text(soup: BeautifulSoup) -> str:
        """Extract main article text as reliably as possible.

        Einfomax pages sometimes include a lot of navigation/boilerplate; also some environments
        may receive a 'block/interstitial' HTML with 200. We try common article containers first.
        """
        candidates = [
            "div#article-view-content-div",          # common on many Korean news CMS
            "div#articleBody",                      # fallback
            "section#article-view-content-div",     # variant
            "div.article-body",                     # generic
            "div.view_cont",                        # generic
            "article",                              # last resort
        ]
        for sel in candidates:
            el = soup.select_one(sel)
            if el:
                txt = el.get_text("\n", strip=True)
                if txt and len(txt) > 200:
                    return txt
        return soup.get_text("\n", strip=True)

    def _debug_context(text: str, keyword: str, width: int = 200) -> str:
        i = text.find(keyword)
        if i < 0:
            return ""
        start = max(0, i - width)
        end = min(len(text), i + len(keyword) + width)
        return text[start:end]

    # ë°°í¬ í™˜ê²½ì—ì„œ ì¢…ì¢… 200ìœ¼ë¡œ ì°¨ë‹¨/ì•ˆë‚´ íŽ˜ì´ì§€ê°€ ë‚´ë ¤ì˜¤ëŠ” ê²½ìš°ê°€ ìžˆì–´, ë³¸ë¬¸ì´ ë¹„ì •ìƒì ìœ¼ë¡œ ì§§ìœ¼ë©´ ì°¨ë‹¨ ì˜ì‹¬
    if len(body_text) < 300:
        status = getattr(article_res, "status_code", "n/a")
        print("[EXPECTED_RANGE] article page status=", status)
        print("[EXPECTED_RANGE] article page snippet=", body_text[:400])

    # Railwayì—ì„œë§Œ ìž¬í˜„ë˜ëŠ” 'ì •ìƒ 200ì¸ë° ë‚´ìš©ì´ ë‹¤ë¥¸' ì¼€ì´ìŠ¤ë¥¼ ë¹ ë¥´ê²Œ íŒë³„
    if ("ì ‘ê·¼" in body_text and "ì°¨ë‹¨" in body_text) or ("Forbidden" in body_text) or ("Cloudflare" in body_text):
        print("[EXPECTED_RANGE] âš ï¸ possible block/interstitial page detected")

    # 4. ê¸°ì‚¬ ë‚ ì§œ í™•ì¸
    meta_time = article_soup.find("meta", {"property": "article:published_time"})
    content = meta_time.get("content") if meta_time else None
    if not content:
        # fallback: try other common meta/name fields
        meta_alt = (
            article_soup.find("meta", {"name": "article:published_time"})
            or article_soup.find("meta", {"name": "pubdate"})
            or article_soup.find("meta", {"property": "og:updated_time"})
        )
        content = meta_alt.get("content") if meta_alt else None

    if not content:
        raise ValueError("âŒ ê¸°ì‚¬ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    article_date = datetime.strptime(content.split("T")[0], "%Y-%m-%d").date()

    today = datetime.now(pytz.timezone("Asia/Seoul")).date()
    if article_date != today:
        raise ValueError(f"ðŸ“… ì˜¤ëŠ˜ ê¸°ì‚¬ ì•„ë‹˜: {article_date}")

    # 5. ì „ì²´ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    full_text = body_text

    # 6. ì •ê·œì‹ìœ¼ë¡œ ì˜ˆìƒ ë ˆì¸ì§€ ì¶”ì¶œ (í‘œê¸° ë³€í˜• ëŒ€ì‘)
    # - 'ì˜ˆìƒ ë ˆì¸ì§€', 'ì˜ˆìƒë ˆì¸ì§€', 'ì˜ˆìƒ ë²”ìœ„', 'ì˜ˆìƒí™˜ìœ¨ ë ˆì¸ì§€' ë“±
    # - êµ¬ë¶„ìž: ~, -, â€“
    # - ë‹¨ìœ„: 'ì›' ìœ ë¬´
    patterns = [
        r"ì˜ˆìƒ\s*(?:í™˜ìœ¨\s*)?(?:ë ˆì¸ì§€|ë²”ìœ„)\s*[:ï¼š]?\s*([\d,\.]+)\s*[~\-â€“]\s*([\d,\.]+)\s*ì›?",
        r"(?:ë ˆì¸ì§€|ë²”ìœ„)\s*[:ï¼š]?\s*([\d,\.]+)\s*[~\-â€“]\s*([\d,\.]+)\s*ì›?\s*(?:ë¡œ|ìœ¼ë¡œ)?\s*ì˜ˆìƒ",
    ]

    range_matches = []
    for pat in patterns:
        found = re.findall(pat, full_text)
        if found:
            range_matches.extend(found)

    if not range_matches:
        # Debug: Railwayì—ì„œ ë³¸ë¬¸ì€ ë°›ì•„ì™”ëŠ”ë° í‚¤ì›Œë“œ/í˜•ì‹ì´ ë‹¬ë¼ ì‹¤íŒ¨í•˜ëŠ” ì¼€ì´ìŠ¤
        ctx1 = _debug_context(full_text, "ì˜ˆìƒ", 250)
        ctx2 = _debug_context(full_text, "ë ˆì¸ì§€", 250)
        ctx3 = _debug_context(full_text, "ë²”ìœ„", 250)
        print("[EXPECTED_RANGE] âŒ regex miss - contexts:")
        if ctx1:
            print("[EXPECTED_RANGE] ...around 'ì˜ˆìƒ'...\n", ctx1)
        if ctx2:
            print("[EXPECTED_RANGE] ...around 'ë ˆì¸ì§€'...\n", ctx2)
        if ctx3:
            print("[EXPECTED_RANGE] ...around 'ë²”ìœ„'...\n", ctx3)

        # Fallback: í‚¤ì›Œë“œê°€ ìžˆëŠ” ê²½ìš°, ì£¼ë³€ì—ì„œ 'ìˆ«ìž~ìˆ«ìž' í˜•íƒœë¥¼ í•œ ë²ˆ ë” íƒìƒ‰
        window_text = "\n".join([t for t in [ctx1, ctx2, ctx3] if t]) or full_text
        fallback = re.findall(r"([\d,\.]{3,})\s*[~\-â€“]\s*([\d,\.]{3,})", window_text)
        if fallback:
            range_matches = fallback
        else:
            raise ValueError("âŒ ì˜ˆìƒ í™˜ìœ¨ ë²”ìœ„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 7. ì‰¼í‘œ ì œê±° ë° float ë³€í™˜
    ranges = []
    for low, high in range_matches:
        try:
            low_clean = float(low.replace(",", ""))
            high_clean = float(high.replace(",", ""))
            ranges.append((low_clean, high_clean))
        except ValueError:
            continue

    if not ranges:
        raise ValueError("âŒ ìœ íš¨í•œ ìˆ«ìž í˜•ì‹ì˜ ë²”ìœ„ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # 8. ê°€ìž¥ ë„“ì€ ë²”ìœ„ ê³„ì‚°
    low = min(l for l, _ in ranges)
    high = max(h for _, h in ranges)

    print("âœ… ìŠ¤í¬ëž˜í•‘ëœ ì˜ˆìƒ í™˜ìœ¨ ë ˆì¸ì§€:", ranges)

    return {
        "date": today,
        "low": low,
        "high": high,
        "source": article_url,
    }